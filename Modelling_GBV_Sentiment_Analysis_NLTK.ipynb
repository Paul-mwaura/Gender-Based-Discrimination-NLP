{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Updated Modelling - GBV_Sentiment Analysis_NLTK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sylviawanjiku/Gender-Based-Discrimination-NLP/blob/main/Modelling_GBV_Sentiment_Analysis_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8OOZpk8Eczv"
      },
      "source": [
        "## GBV Sentiment Analysis \n",
        "\n",
        "Lets get the sentiment and polarity of the tweets-attitude or emotion of thetext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHVfaQsZm-g3"
      },
      "source": [
        "Short Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp0kvmQ5EZlc"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3806-VjnYrHB"
      },
      "source": [
        "%%capture \n",
        "#Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Installing NLTK\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpNp62xSEXIa"
      },
      "source": [
        "### Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U03PNDBwatrP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "3961a49f-2b01-404b-b2c3-89bc8d08c3b2"
      },
      "source": [
        "df = pd.read_csv(\"GBV_data_clean_v01.csv\", error_bad_lines=False)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(46670, 8)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>username</th>\n",
              "      <th>new_tweets</th>\n",
              "      <th>clean_tweets</th>\n",
              "      <th>tweets_without_stopwords</th>\n",
              "      <th>replies</th>\n",
              "      <th>retweets</th>\n",
              "      <th>likes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>shaybspears</td>\n",
              "      <td>We should talk more about how this dangerous w...</td>\n",
              "      <td>we should talk more about how this dangerous w...</td>\n",
              "      <td>talk dangerous woman stalking britney sending ...</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>kaylanicole9991</td>\n",
              "      <td>And no, getting me drunk wonât work. After w...</td>\n",
              "      <td>and no getting me drunk won  t work after what...</td>\n",
              "      <td>getting drunk work happened male best friend j...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>marge78355824</td>\n",
              "      <td>Itâs not just about seeing a random penis...</td>\n",
              "      <td>it  s not just about seeing a random penis ...</td>\n",
              "      <td>seeing random penis fear every woman sexual vi...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>explorevenango</td>\n",
              "      <td>A Franklin woman who left multiple messages in...</td>\n",
              "      <td>a franklin woman who left multiple messages in...</td>\n",
              "      <td>franklin woman left multiple messages attempt ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>politikscommon</td>\n",
              "      <td>I did read the details. Walker was also sue...</td>\n",
              "      <td>i did read the details walker was also sued...</td>\n",
              "      <td>read details walker also sued woman accused wa...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0         username  ... retweets likes\n",
              "0           0      shaybspears  ...       14    27\n",
              "1           1  kaylanicole9991  ...        0     0\n",
              "2           2    marge78355824  ...        0     1\n",
              "3           3   explorevenango  ...        0     0\n",
              "4           4   politikscommon  ...        0     0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UuXzu1ipxD7"
      },
      "source": [
        "df.dropna(inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Spvjge4aqc9o",
        "outputId": "36dd089e-0e50-45f8-e395-4c18ce6ca243"
      },
      "source": [
        "\n",
        "df.head(30)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>username</th>\n",
              "      <th>new_tweets</th>\n",
              "      <th>clean_tweets</th>\n",
              "      <th>tweets_without_stopwords</th>\n",
              "      <th>replies</th>\n",
              "      <th>retweets</th>\n",
              "      <th>likes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>shaybspears</td>\n",
              "      <td>We should talk more about how this dangerous w...</td>\n",
              "      <td>we should talk more about how this dangerous w...</td>\n",
              "      <td>talk dangerous woman stalking britney sending ...</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>kaylanicole9991</td>\n",
              "      <td>And no, getting me drunk wonât work. After w...</td>\n",
              "      <td>and no getting me drunk won  t work after what...</td>\n",
              "      <td>getting drunk work happened male best friend j...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>marge78355824</td>\n",
              "      <td>Itâs not just about seeing a random penis...</td>\n",
              "      <td>it  s not just about seeing a random penis ...</td>\n",
              "      <td>seeing random penis fear every woman sexual vi...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>explorevenango</td>\n",
              "      <td>A Franklin woman who left multiple messages in...</td>\n",
              "      <td>a franklin woman who left multiple messages in...</td>\n",
              "      <td>franklin woman left multiple messages attempt ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>politikscommon</td>\n",
              "      <td>I did read the details. Walker was also sue...</td>\n",
              "      <td>i did read the details walker was also sued...</td>\n",
              "      <td>read details walker also sued woman accused wa...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>lulabelldesigns</td>\n",
              "      <td>ALERT: Devin Nunes needs your help ASAP. We MU...</td>\n",
              "      <td>alert devin nunes needs your help asap we must...</td>\n",
              "      <td>alert devin nunes needs help asap must defend ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>cole_wex</td>\n",
              "      <td>It's amazing that when you want to defend C...</td>\n",
              "      <td>its amazing that when you want to defend ch...</td>\n",
              "      <td>amazing want defend chauncey subject ask defen...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>blunthonesty4</td>\n",
              "      <td>Last I heard unfortunately it's similar in ...</td>\n",
              "      <td>last i heard unfortunately its similar in a...</td>\n",
              "      <td>last heard unfortunately similar aus woman rap...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>seanrosssapp</td>\n",
              "      <td>Oh, you're talking about the guy who implied...</td>\n",
              "      <td>oh youre talking about the guy who implied t...</td>\n",
              "      <td>oh youre talking guy implied illegitimate chil...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>menopausemum</td>\n",
              "      <td>Well I donât see why we should pay him a p...</td>\n",
              "      <td>well i don  t see why we should pay him a pe...</td>\n",
              "      <td>well see pay penny groping side chicks assbut ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>roseygoodman</td>\n",
              "      <td>way to #VictimBlame. A woman gets assaulted b...</td>\n",
              "      <td>way to victimblame a woman gets assaulted by ...</td>\n",
              "      <td>way victimblame woman gets assaulted maskhole ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>imasample13</td>\n",
              "      <td>It was reported that Floyd was the assailan...</td>\n",
              "      <td>it was reported that floyd was the assailan...</td>\n",
              "      <td>reported floyd assailant crime involving viole...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>2keercous</td>\n",
              "      <td>Didn't he sxually assault a woman</td>\n",
              "      <td>didnt he sxually assault a woman</td>\n",
              "      <td>didnt sxually assault woman</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>notalottamani</td>\n",
              "      <td>You played the role of the âundesirableâ l...</td>\n",
              "      <td>you played the role of the   undesirable   lou...</td>\n",
              "      <td>played role undesirable loud brazen black woma...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>colinfry666</td>\n",
              "      <td>I think you deserve a highly-paid advisor rol...</td>\n",
              "      <td>i think you deserve a highlypaid advisor role...</td>\n",
              "      <td>think deserve highlypaid advisor role hancock ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>fakesghostin</td>\n",
              "      <td>men thinks that womens generalise men for a ra...</td>\n",
              "      <td>men thinks that womens generalise men for a ra...</td>\n",
              "      <td>men thinks womens generalise men rape sexual a...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>ehholiviaa</td>\n",
              "      <td>not this woman trying to excuse assault.</td>\n",
              "      <td>not this woman trying to excuse assault</td>\n",
              "      <td>woman trying excuse assault</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>succubus_pride</td>\n",
              "      <td>â£ï¸: Okay so- As true as this is in recent...</td>\n",
              "      <td>okay so as true as this is in recent years...</td>\n",
              "      <td>okay true recent years cant use mens r statist...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>llarim9</td>\n",
              "      <td>ummm this nigga wrote a song ab stalking &amp;amp;...</td>\n",
              "      <td>ummm this nigga wrote a song ab stalking  rpin...</td>\n",
              "      <td>ummm nigga wrote song ab stalking rping woman</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>ben_ac_elliott</td>\n",
              "      <td>They're so utterly made insane by inane nonse...</td>\n",
              "      <td>theyre so utterly made insane by inane nonsen...</td>\n",
              "      <td>theyre utterly made insane inane nonsense grow...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>dukeoriordan</td>\n",
              "      <td>Married man who commits adultery with a marrie...</td>\n",
              "      <td>married man who commits adultery with a marrie...</td>\n",
              "      <td>married man commits adultery married woman put...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>qshironalbertie</td>\n",
              "      <td>I feel for any woman (anyone) who experienced...</td>\n",
              "      <td>i feel for any woman anyone who experienced a...</td>\n",
              "      <td>feel woman anyone experienced sexual physical ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>marinersilas</td>\n",
              "      <td>If Freddie Mercury walked into the women's se...</td>\n",
              "      <td>if freddie mercury walked into the womens sec...</td>\n",
              "      <td>freddie mercury walked womens section guy whos...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>nogoodchuck2</td>\n",
              "      <td>White woman, admits to stalking and harassing ...</td>\n",
              "      <td>white woman admits to stalking and harassing a...</td>\n",
              "      <td>white woman admits stalking harassing black wo...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>twistedtwinsin1</td>\n",
              "      <td>â competition, clever, the other woman. â ...</td>\n",
              "      <td>competition clever the other woman    heart...</td>\n",
              "      <td>competition clever woman heartbreak pain separ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>msmimz1</td>\n",
              "      <td>âMimz Mimz Mimz Mimz Mimz Mimzâ.   I can ...</td>\n",
              "      <td>mimz mimz mimz mimz mimz mimz     i can ima...</td>\n",
              "      <td>mimz mimz mimz mimz mimz mimz imagine officer ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>imshanereaction</td>\n",
              "      <td>And for the love of God please no more pictur...</td>\n",
              "      <td>and for the love of god please no more pictur...</td>\n",
              "      <td>love god please pictures man groping woman lik...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>28</td>\n",
              "      <td>bgwritesstuff</td>\n",
              "      <td>I love New York. We will ALL take the time out...</td>\n",
              "      <td>i love new york we will all take the time out ...</td>\n",
              "      <td>love new york take time busy day beat assault ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>nayomss4dcash</td>\n",
              "      <td>You slyed our date to be stalking woman abi</td>\n",
              "      <td>you slyed our date to be stalking woman abi</td>\n",
              "      <td>slyed date stalking woman abi</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30</td>\n",
              "      <td>tracyrwyatt</td>\n",
              "      <td>This individual has been harassing and sta...</td>\n",
              "      <td>this individual has been harassing and sta...</td>\n",
              "      <td>individual harassing stalking woman year time ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Unnamed: 0         username  ... retweets likes\n",
              "0            0      shaybspears  ...       14    27\n",
              "1            1  kaylanicole9991  ...        0     0\n",
              "2            2    marge78355824  ...        0     1\n",
              "3            3   explorevenango  ...        0     0\n",
              "4            4   politikscommon  ...        0     0\n",
              "5            5  lulabelldesigns  ...        1     3\n",
              "6            6         cole_wex  ...        0     1\n",
              "7            7    blunthonesty4  ...        0     6\n",
              "8            8     seanrosssapp  ...        0     2\n",
              "9            9     menopausemum  ...        0     0\n",
              "10          10     roseygoodman  ...        0     0\n",
              "11          11      imasample13  ...        0     0\n",
              "12          12        2keercous  ...        0     0\n",
              "13          13    notalottamani  ...        2    16\n",
              "14          14      colinfry666  ...        0     1\n",
              "15          15     fakesghostin  ...        0     0\n",
              "16          16       ehholiviaa  ...        0     1\n",
              "17          17   succubus_pride  ...        0     0\n",
              "18          18          llarim9  ...        0     0\n",
              "19          19   ben_ac_elliott  ...        3    66\n",
              "20          20     dukeoriordan  ...        0     1\n",
              "21          21  qshironalbertie  ...        0     0\n",
              "22          22     marinersilas  ...        1     0\n",
              "23          23     nogoodchuck2  ...        1     5\n",
              "24          24  twistedtwinsin1  ...        0     1\n",
              "25          25          msmimz1  ...        0     1\n",
              "26          27  imshanereaction  ...        0    14\n",
              "27          28    bgwritesstuff  ...        0     1\n",
              "28          29    nayomss4dcash  ...        1     2\n",
              "29          30      tracyrwyatt  ...        0     1\n",
              "\n",
              "[30 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c98I93kDvQ7p"
      },
      "source": [
        "#drop Nas\n",
        "df.dropna(inplace=True, axis=0)\n",
        "df.isna().sum()\n",
        "tweet_df=df.copy()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "yhD41WgJvfY5",
        "outputId": "85118b49-a182-4a7c-8785-521cc744dc8f"
      },
      "source": [
        "df['clean_tweets'][0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'we should talk more about how this dangerous woman was stalking britney sending her creepy emails about   a gay demon   and britney reported her to her lawyer but instead of issuing a restraining order against her jamie spears made her brit'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSIIaarkWD1h"
      },
      "source": [
        "Using NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA_3aNLZYeqM"
      },
      "source": [
        "NLTK already has a built-in, pretrained sentiment analyzer called VADER (Valence Aware Dictionary and sEntiment Reasoner).\n",
        "\n",
        "Since VADER is pretrained, you can get results more quickly than with many other analyzers. However, VADER is best suited for language used in social media, like short sentences with some slang and abbreviations. It’s less accurate when rating longer, structured sentences, but it’s often a good launching point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOVoFUQNWMq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71fa79b3-07f8-4a08-e04a-3e4e141fc5b5"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download([\n",
        "\"names\",\n",
        " \"stopwords\",\n",
        " \"state_union\",\n",
        " \"twitter_samples\",\n",
        "\"movie_reviews\",\n",
        " \"averaged_perceptron_tagger\",\n",
        " \"vader_lexicon\",\n",
        "\"punkt\",\n",
        " ])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeYgN1MzY6qK",
        "outputId": "28e5207b-a643-4a07-c736-f68f0489e2da"
      },
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sia.polarity_scores(\"Wow, NLTK is really powerful!\")\n",
        "#{'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.8012}"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'compound': 0.8012, 'neg': 0.0, 'neu': 0.295, 'pos': 0.705}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv-YisgKc8Iu"
      },
      "source": [
        "pol=[]\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "for  tw in df['tweets_without_stopwords']:\n",
        "  result=sia.polarity_scores(tw)\n",
        "  pol.append(result)\n",
        "df['pol_nltk']=pol\n",
        " "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjIBuMzzB6V7"
      },
      "source": [
        "nlt=[]\n",
        "for k in df['pol_nltk']:\n",
        "  k.pop(\"compound\",None)\n",
        "  inverse = [(value, key) for key, value in k.items()]\n",
        "  n=max(inverse)[1]\n",
        "  nlt.append(n)\n",
        "df['nltk_sentiment']=nlt"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw8sKCuSftkn",
        "outputId": "42b596ae-0d7c-43e8-a32f-95c0a05cf2ed"
      },
      "source": [
        "#df.sample(50)\n",
        "df['nltk_sentiment'].value_counts()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neu    38061\n",
              "neg     6734\n",
              "pos     1377\n",
              "Name: nltk_sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A2q4tEZqOpj"
      },
      "source": [
        "#selecting a random sample of 1377 tweets of ne,neg and positive\n",
        "grouped = df.groupby('nltk_sentiment')\n",
        "pos_tweets= df.loc[df['nltk_sentiment']=='pos']\n",
        "neg_tweets= df.loc[df['nltk_sentiment']=='neg']\n",
        "neu_tweets= df.loc[df['nltk_sentiment']=='neu']\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltQYmw5famoU",
        "outputId": "840869ef-26b0-4063-fe5c-92361ead3a47"
      },
      "source": [
        "#set seed\n",
        "np.random.seed(1000)\n",
        "#random selection of 1377 tweets per group\n",
        "neg_sample=neg_tweets.sample(1377)\n",
        "neg_sample.shape\n",
        "\n",
        "neu_sample=neu_tweets.sample(1377)\n",
        "neu_sample.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1377, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_AbjuSgeCcE",
        "outputId": "5fc53110-7c06-4ece-83fd-3a62f39362e2"
      },
      "source": [
        "#combine the three to create a new data frame\n",
        "df_sample=pos_tweets.append([neg_sample,neu_sample])\n",
        "df_sample.head(5)\n",
        "df_sample.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4131, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjUmrOprgRNh",
        "outputId": "c784e3e0-f932-4d02-fb59-b6e1817f709a"
      },
      "source": [
        "#confirm the counts\n",
        "df_sample['nltk_sentiment'].value_counts()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pos    1377\n",
              "neu    1377\n",
              "neg    1377\n",
              "Name: nltk_sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaeRKL8hggw4"
      },
      "source": [
        "We have managed to get a sample of 1377 tweets for each sentiment, lets see how our model behaves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3gm0Im7gBeEy",
        "outputId": "e7e7e3b3-1974-4fe7-b597-ff11223dbfde"
      },
      "source": [
        "#divide the data and save\n",
        "df_sample.to_csv(\"GBV_data_clean_v04.csv\")\n",
        "from google.colab import files\n",
        "files.download('GBV_data_clean_v04.csv')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_dde074b2-fdde-4384-8e53-529e78dbf0c2\", \"GBV_data_clean_v04.csv\", 1916673)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l2sbrsHWJoG"
      },
      "source": [
        "## Finding a Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roPEIMTuffJz"
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MuuMncI0c2jn",
        "outputId": "817cb1c5-4f60-4a2d-e078-e8b4880389d1"
      },
      "source": [
        "data = df_sample[['tweets_without_stopwords', 'replies', 'retweets', 'likes', 'nltk_sentiment']]\n",
        "data.columns = ['tweets', 'replies', 'retweets', 'likes', 'nltk_sentiment']\n",
        "data.head()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>replies</th>\n",
              "      <th>retweets</th>\n",
              "      <th>likes</th>\n",
              "      <th>nltk_sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>didnt sxually assault woman</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>love god please pictures man groping woman lik...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>lol</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>agree cant attack assault someone exactly woma...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>dm ok said seem like lovely man lets chat</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                tweets  ... nltk_sentiment\n",
              "12                         didnt sxually assault woman  ...            pos\n",
              "26   love god please pictures man groping woman lik...  ...            pos\n",
              "64                                                 lol  ...            pos\n",
              "77   agree cant attack assault someone exactly woma...  ...            pos\n",
              "121          dm ok said seem like lovely man lets chat  ...            pos\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcxdYug3difA"
      },
      "source": [
        "### Split the data into Features and Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBBbx1S8dX7C"
      },
      "source": [
        "# split the dataset into training and validation datasets \n",
        "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(data['tweets'], data['nltk_sentiment'])\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_valid = encoder.fit_transform(y_valid)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSzdmU1Km5-g"
      },
      "source": [
        "### Count Vectors as features\n",
        "Count Vector is a matrix notation of the dataset in which every row represents a document from the corpus, every column represents a term from the corpus, and every cell represents the frequency count of a particular term in a particular document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkQDGIWtm9_z"
      },
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(data['tweets'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(X_train)\n",
        "xvalid_count =  count_vect.transform(X_valid)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpoGfT3giM_j"
      },
      "source": [
        "### TF-IDF Vectors as features\n",
        ">>\n",
        "* TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
        "* IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
        "\n",
        ">>\n",
        "TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams)\n",
        "* a. Word Level TF-IDF : Matrix representing tf-idf scores of every term in different documents\n",
        "* b. N-gram Level TF-IDF : N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams\n",
        "* c. Character Level TF-IDF : Matrix representing tf-idf scores of character level n-grams in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9icPvqE6f54K"
      },
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(data['tweets'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(X_train)\n",
        "xvalid_tfidf =  tfidf_vect.transform(X_valid)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(data['tweets'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_valid)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(data['tweets'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_valid) "
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnEdwcFRi3sS"
      },
      "source": [
        "### Word Embeddings\n",
        "A word embedding is a form of representing words and documents using a dense vector representation. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n",
        "There are four essential steps for using word embeddings:\n",
        "* Loading the pretrained word embeddings\n",
        "* Creating a tokenizer object\n",
        "* Transforming text documents to sequence of tokens and pad them\n",
        "* Create a mapping of token and their respective embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3WkNv-qigNC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "24546e5c-777a-4a7a-b1d1-37f292f06a7a"
      },
      "source": [
        "# # load the pre-trained word-embedding vectors \n",
        "# embeddings_index = {}\n",
        "# for i, line in enumerate(open('data/wiki-news-300d-1M.vec')):\n",
        "#     values = line.split()\n",
        "#     embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(data['tweets'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=70)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_valid), maxlen=70)\n",
        "\n",
        "# # create token-embedding mapping\n",
        "# embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-1061bd83731c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# create a tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW7OwctRlEYe"
      },
      "source": [
        "### Text / NLP based features\n",
        "A number of extra text based features can also be created which sometimes are helpful for improving text classification models. Some examples are:\n",
        "1. Word Count of the documents – total number of words in the documents\n",
        "2. Character Count of the documents – total number of characters in the documents\n",
        "3. Average Word Density of the documents – average length of the words used in the documents\n",
        "4. Puncutation Count in the Complete Essay – total number of punctuation marks in the documents\n",
        "5. Upper Case Count in the Complete Essay – total number of upper count words in the documents\n",
        "6. Title Word Count in the Complete Essay – total number of proper case (title) words in the documents\n",
        "7. Frequency distribution of Part of Speech Tags:\n",
        ">\n",
        "* Noun Count\n",
        "* Verb Count\n",
        "* Adjective Count\n",
        "* Adverb Count\n",
        "* Pronoun Count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er9ZlOpujfTy"
      },
      "source": [
        "data['char_count'] = data['tweets'].apply(len)\n",
        "data['word_count'] = data['tweets'].apply(lambda x: len(x.split()))\n",
        "data['word_density'] = data['char_count'] / (data['word_count']+1)\n",
        "data['punctuation_count'] = data['tweets'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "data['title_word_count'] = data['tweets'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "data['upper_case_word_count'] = data['tweets'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W96BDjckknIx"
      },
      "source": [
        "pos_family = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "# function to check and get the part of speech tag count of a words in a given sentence\n",
        "def check_pos_tag(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = textblob.TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_family[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz8lVuvTkvUC"
      },
      "source": [
        "data['noun_count'] = data['tweets'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
        "data['verb_count'] = data['tweets'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
        "data['adj_count'] = data['tweets'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
        "data['adv_count'] = data['tweets'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
        "data['pron_count'] = data['tweets'].apply(lambda x: check_pos_tag(x, 'pron'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "xvuML8kTl8-Y",
        "outputId": "ce28b316-a787-4d51-a652-3f8983f2015d"
      },
      "source": [
        "data.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>replies</th>\n",
              "      <th>retweets</th>\n",
              "      <th>likes</th>\n",
              "      <th>nltk_sentiment</th>\n",
              "      <th>char_count</th>\n",
              "      <th>word_count</th>\n",
              "      <th>word_density</th>\n",
              "      <th>punctuation_count</th>\n",
              "      <th>title_word_count</th>\n",
              "      <th>upper_case_word_count</th>\n",
              "      <th>noun_count</th>\n",
              "      <th>verb_count</th>\n",
              "      <th>adj_count</th>\n",
              "      <th>adv_count</th>\n",
              "      <th>pron_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>talk dangerous woman stalking britney sending ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>neu</td>\n",
              "      <td>150</td>\n",
              "      <td>21</td>\n",
              "      <td>6.818182</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>getting drunk work happened male best friend j...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neu</td>\n",
              "      <td>141</td>\n",
              "      <td>21</td>\n",
              "      <td>6.409091</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>seeing random penis fear every woman sexual vi...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>neu</td>\n",
              "      <td>101</td>\n",
              "      <td>16</td>\n",
              "      <td>5.941176</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              tweets  ...  pron_count\n",
              "0  talk dangerous woman stalking britney sending ...  ...           0\n",
              "1  getting drunk work happened male best friend j...  ...           0\n",
              "2  seeing random penis fear every woman sexual vi...  ...           0\n",
              "\n",
              "[3 rows x 16 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fB3Hx5Gl7n_"
      },
      "source": [
        "### Topic Models as features\n",
        ">>\n",
        "Topic Modelling is a technique to identify the groups of words (called a topic) from a collection of documents that contains best information in the collection. \n",
        ">>\n",
        "I have used Latent Dirichlet Allocation for generating Topic Modelling Features. LDA is an iterative model which starts from a fixed number of topics. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics. \n",
        ">>\n",
        "Although the tokens themselves are meaningless, the probability distributions over words provided by the topics provide a sense of the different ideas contained in the documents. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkhpmA4Kk-Qp"
      },
      "source": [
        "# train a LDA Model\n",
        "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
        "X_topics = lda_model.fit_transform(xtrain_count)\n",
        "topic_word = lda_model.components_ \n",
        "vocab = count_vect.get_feature_names()\n",
        "\n",
        "# view the topic models\n",
        "n_top_words = 10\n",
        "topic_summaries = []\n",
        "for i, topic_dist in enumerate(topic_word):\n",
        "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
        "    topic_summaries.append(' '.join(topic_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6kLPMFUodZI"
      },
      "source": [
        "## Model Building\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3W110NjmexP"
      },
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    if is_neural_net:\n",
        "        predictions = predictions.argmax(axis=-1)\n",
        "    \n",
        "    return metrics.accuracy_score(predictions, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Kv3_lEJopAI"
      },
      "source": [
        "### Naive Bayes\n",
        "Naive Bayes is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjJxPrmyolqJ",
        "outputId": "9e0a0490-1237-41e7-e0e4-153e35d7221a"
      },
      "source": [
        "# Naive Bayes\n",
        "print(\"Naive Bayes\\n\")\n",
        "# Naive Bayes on Count Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, y_train, xvalid_count)\n",
        "print(\"Count Vectors: \", accuracy*100)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, y_train, xvalid_tfidf)\n",
        "print(\"WordLevel TF-IDF: \", accuracy*100)\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, y_train, xvalid_tfidf_ngram)\n",
        "print(\"N-Gram Vectors: \", accuracy*100)\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, y_train, xvalid_tfidf_ngram_chars)\n",
        "print(\"CharLevel Vectors: \", accuracy*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes\n",
            "\n",
            "Count Vectors:  93.30922242314648\n",
            "WordLevel TF-IDF:  94.9367088607595\n",
            "N-Gram Vectors:  95.84086799276673\n",
            "CharLevel Vectors:  94.9367088607595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6vZDZ5epXfV"
      },
      "source": [
        "### Linear Classifier (Logistic Regression)\n",
        "Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic/sigmoid function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMheTLzqo6J6",
        "outputId": "4b7b8d60-fbc7-4ce0-8edd-c87c3497bd8f"
      },
      "source": [
        "# Linear Regression \n",
        "print(\"Linear Regression\\n\")\n",
        "# Linear Classifier on Count Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, y_train, xvalid_count)\n",
        "print(\"Count Vectors: \", accuracy*100)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, y_train, xvalid_tfidf)\n",
        "print(\"WordLevel TF-IDF: \", accuracy*100)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, y_train, xvalid_tfidf_ngram)\n",
        "print(\"N-Gram Vectors: \", accuracy*100)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, y_train, xvalid_tfidf_ngram_chars)\n",
        "print(\"CharLevel Vectors: \", accuracy*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression\n",
            "\n",
            "Count Vectors:  96.02169981916818\n",
            "WordLevel TF-IDF:  96.02169981916818\n",
            "N-Gram Vectors:  95.84086799276673\n",
            "CharLevel Vectors:  96.02169981916818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xILR3ZVt1YY"
      },
      "source": [
        "### Implementing a SVM Model\n",
        "Support Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. The model extracts a best possible hyper-plane / line that segregates the two classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsqZnPCFpvnw",
        "outputId": "c21b438a-895c-4944-e599-ef211fac08c1"
      },
      "source": [
        "# SVM on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, y_train, xvalid_tfidf_ngram)\n",
        "print(\"SVM, N-Gram Vectors: \", accuracy*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM, N-Gram Vectors:  95.66003616636529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsIHSYEOvusG"
      },
      "source": [
        "### Bagging Model (Random Forest Model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1iJvbySuBAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24c02733-e6dc-466d-b3cc-ace04a81ebb0"
      },
      "source": [
        "# Random Forest Model\n",
        "print(\"Random Forest Model\")\n",
        "# RF on Count Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, y_train, xvalid_count)\n",
        "print(\"Count Vectors: \", accuracy*100)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, y_train, xvalid_tfidf)\n",
        "print(\"WordLevel TF-IDF: \", accuracy*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Model\n",
            "Count Vectors:  95.84086799276673\n",
            "WordLevel TF-IDF:  96.20253164556962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgB3z0ObwG7F"
      },
      "source": [
        "### Boosting Model (Xtreme Gradient Boosting Model)\n",
        "Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. \n",
        "\n",
        "A weak learner is defined to be a classifier that is only slightly correlated with the true classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTtUZ9ngv-fX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee943e22-39b8-4af0-f95f-629354b614ef"
      },
      "source": [
        "# XGB Model\n",
        "print(\"XGB Model\\n\")\n",
        "# Extereme Gradient Boosting on Count Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), y_train, xvalid_count.tocsc())\n",
        "print(\"Count Vectors: \", accuracy*100)\n",
        "\n",
        "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), y_train, xvalid_tfidf.tocsc())\n",
        "print(\"WordLevel TF-IDF: \", accuracy*100)\n",
        "\n",
        "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), y_train, xvalid_tfidf_ngram_chars.tocsc())\n",
        "print(\"CharLevel Vectors: \", accuracy*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB Model\n",
            "\n",
            "Count Vectors:  95.84086799276673\n",
            "WordLevel TF-IDF:  96.20253164556962\n",
            "CharLevel Vectors:  96.20253164556962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NTmOX1Jxl_Q"
      },
      "source": [
        "### Shallow Neural Networks\n",
        "A neural network is a mathematical model that is designed to behave similar to biological neurons and nervous system. \n",
        "\n",
        "These models are used to recognize complex patterns and relationships that exists within a labelled data. \n",
        "\n",
        "A shallow neural network contains mainly three types of layers – input layer, hidden layer, and output layer. \n",
        "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/OH3gI-1.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_-GdHxqxO-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f50076b-2f8d-43ac-a7c2-6ec77b4b044f"
      },
      "source": [
        "def create_model_architecture(input_size):\n",
        "    # create input layer \n",
        "    input_layer = layers.Input((input_size, ), sparse=True)\n",
        "    \n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(128, activation=\"relu\")(input_layer)\n",
        "    hidden_layer = layers.Dense(256, activation=\"relu\")(hidden_layer)\n",
        "    \n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy')\n",
        "    return classifier \n",
        "\n",
        "print(\"Neural Network Model\\n\")\n",
        "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
        "accuracy = train_model(classifier, xtrain_tfidf_ngram, y_train, xvalid_tfidf_ngram, is_neural_net=True)\n",
        "print(\"Ngram Level TF IDF Vectors\",  accuracy*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network Model\n",
            "\n",
            "52/52 [==============================] - 1s 7ms/step - loss: 0.3580\n",
            "Ngram Level TF IDF Vectors 4.882459312839059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UCucgZt3pNt"
      },
      "source": [
        "### Deep Neural Networks\n",
        "Deep Neural Networks are more complex neural networks in which the hidden layers performs much more complex operations than simple sigmoid or relu activations. Different types of deep learning models can be applied in text classification problems.\n",
        "\n",
        "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/OH3gI.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ryEGPJ3z6a"
      },
      "source": [
        "#### Convolutional Neural Network\n",
        "In Convolutional neural networks, convolutions over the input layer are used to compute the output. This results in local connections, where each region of the input is connected to a neuron in the output. Each layer applies different filters and combines their results.\n",
        "\n",
        "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/cnnimage.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM97Nj0kyOrW"
      },
      "source": [
        "# def create_cnn():\n",
        "#     # Add an Input Layer\n",
        "#     input_layer = layers.Input((70, ))\n",
        "\n",
        "#     # Add the word embedding Layer\n",
        "#     embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "#     embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "#     # Add the convolutional Layer\n",
        "#     conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "#     # Add the pooling Layer\n",
        "#     pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "#     # Add the output Layers\n",
        "#     output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "#     output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "#     output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "#     # Compile the model\n",
        "#     model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "#     model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "#     return model\n",
        "\n",
        "# # CNN Model\n",
        "# print(\"CNN Model\\n\")\n",
        "# classifier = create_cnn()\n",
        "# accuracy = train_model(classifier, train_seq_x, y_train, valid_seq_x, is_neural_net=True)\n",
        "# print(\"Word Embeddings\",  accuracy*100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd5EsOJY4UYp"
      },
      "source": [
        "### Recurrent Neural Network – LSTM\n",
        "Unlike Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent Neural Networks. This creates loops in the neural network architecture which acts as a ‘memory state’ of the neurons. This state allows the neurons an ability to remember what have been learned so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J64BnNS4KUC"
      },
      "source": [
        "# def create_rnn_lstm():\n",
        "#     # Add an Input Layer\n",
        "#     input_layer = layers.Input((70, ))\n",
        "\n",
        "#     # Add the word embedding Layer\n",
        "#     embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "#     embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "#     # Add the LSTM Layer\n",
        "#     lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "\n",
        "#     # Add the output Layers\n",
        "#     output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "#     output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "#     output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "#     # Compile the model\n",
        "#     model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "#     model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "#     return model\n",
        "\n",
        "# # RNN-LSTM Model\n",
        "# print(\"RNN-LSTM\\n\")\n",
        "# classifier = create_rnn_lstm()\n",
        "# accuracy = train_model(classifier, train_seq_x, y_train, valid_seq_x, is_neural_net=True)\n",
        "# print(\"Word Embeddings\",  accuracy*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgSHoZ4B44Ai"
      },
      "source": [
        "## Improving Text Classification Models\n",
        "The following are some tips to improve the performance of text classification models and this framework.\n",
        ">>\n",
        "1. Text Cleaning : text cleaning can help to reduce the noise present in text data in the form of stopwords, punctuations marks, suffix variations etc. \n",
        ">>\n",
        "2. Hstacking Text / NLP features with text feature vectors : In the feature engineering section, we generated a number of different feature vectors, combining them together can help to improve the accuracy of the classifier.\n",
        ">>\n",
        "3. Hyperparamter Tuning in modelling : Tuning the parameters is an important step, a number of parameters such as tree length, leafs, network paramters etc can be fine tuned to get a best fit model.\n",
        ">>\n",
        "4. Ensemble Models : Stacking different models and blending their outputs can help to further improve the results. "
      ]
    }
  ]
}